# DA버젼 코드 - 85점/개선버젼1 대비 60/85점

from transformers import AutoTokenizer, AutoModel
import torch
from annoy import AnnoyIndex
import numpy as np

# Step 1: Load BioBERT
model_name = "dmis-lab/biobert-v1.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Function to generate embeddings
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Step 2: Generate Embeddings for your documents
# Assume docs is a list of biomedical text documents
docs = ["Your biomedical text data", "...", "..."]
embeddings = [get_embedding(doc) for doc in docs]

# Step 3: Build Annoy Index
f = embeddings[0].shape[1]  # Length of item vector that will be indexed
t = AnnoyIndex(f, 'angular')

for i, vec in enumerate(embeddings):
    t.add_item(i, vec[0])

t.build(10)  # 10 trees
t.save('test.ann')

# Load index
u = AnnoyIndex(f, 'angular')
u.load('test.ann') 

# Step 4: Querying
query = "Your query text"
query_vec = get_embedding(query)

# Find the nearest neighbors
nns = u.get_nns_by_vector(query_vec[0], 5)  # Find the 5 nearest neighbors

for nn in nns:
    print(docs[nn])

===============================

# DA 개선버전1 - DA버젼 대비 85/60점, DA 개선버전2 대비 80/92점

from transformers import AutoTokenizer, AutoModel
import torch
from annoy import AnnoyIndex
import numpy as np

# Load BioBERT model
def load_model(model_name="dmis-lab/biobert-v1.1"):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        print(f"Error loading model: {e}")
        return None, None

# Generate embeddings
def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Build and save Annoy Index
def build_and_save_annoy_index(embeddings, filename='test.ann', n_trees=10):
    if len(embeddings) == 0:
        print("No embeddings provided.")
        return None

    embedding_dim = embeddings[0].shape[1]
    t = AnnoyIndex(embedding_dim, 'angular')

    for i, vec in enumerate(embeddings):
        t.add_item(i, vec[0])

    t.build(n_trees)
    t.save(filename)
    return t

# Load Annoy Index
def load_annoy_index(filename, embedding_dim):
    u = AnnoyIndex(embedding_dim, 'angular')
    try:
        u.load(filename)
        return u
    except Exception as e:
        print(f"Error loading Annoy index: {e}")
        return None

# Querying function
def query_index(query, tokenizer, model, annoy_index, top_n=5):
    if annoy_index is None:
        print("Annoy index not loaded.")
        return []

    query_vec = get_embedding(query, tokenizer, model)
    nns = annoy_index.get_nns_by_vector(query_vec[0], top_n)
    return nns

# Main workflow
if __name__ == "__main__":
    tokenizer, model = load_model()
    if tokenizer is None or model is None:
        exit(1)

    # Assume docs is a list of biomedical text documents
    docs = ["Your biomedical text data", "...", "..."]
    embeddings = [get_embedding(doc, tokenizer, model) for doc in docs]

    # Build and save the Annoy index
    annoy_file = 'biomed_annoy.ann'
    build_and_save_annoy_index(embeddings, filename=annoy_file)

    # Load the Annoy index
    f = embeddings[0].shape[1]
    u = load_annoy_index(annoy_file, f)
    if u is None:
        exit(1)

    # Querying
    query = "Your query text"
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n=5)
    for nn in nearest_neighbors:
        print(docs[nn])


=====================================

# DA 개선버전2 - DA 개선버젼1 대비 92/80점, DA 개선버젼3 대비 70/90점

from transformers import AutoTokenizer, AutoModel
import torch
from annoy import AnnoyIndex
import numpy as np

# Load BioBERT model
def load_model(model_name="dmis-lab/biobert-v1.1"):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        print(f"Error loading model: {e}")
        return None, None

# Generate embeddings
def get_embeddings(docs, tokenizer, model):
    embeddings = []
    for text in docs:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():  # Ensure no gradient computation to save memory and computations
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())
    return embeddings

# Build and save Annoy Index
def build_and_save_annoy_index(embeddings, filename='test.ann', n_trees=10):
    if not embeddings:
        raise ValueError("No embeddings provided.")

    embedding_dim = embeddings[0].shape[1]
    t = AnnoyIndex(embedding_dim, 'angular')

    for i, vec in enumerate(embeddings):
        t.add_item(i, vec[0])

    t.build(n_trees)
    t.save(filename)
    return t

# Load Annoy Index
def load_annoy_index(filename, embedding_dim):
    u = AnnoyIndex(embedding_dim, 'angular')
    if not u.load(filename):
        raise IOError(f"Could not load Annoy index from {filename}")
    return u

# Querying function
def query_index(query, tokenizer, model, annoy_index, top_n=5):
    query_vec = get_embeddings([query], tokenizer, model)[0]
    nns = annoy_index.get_nns_by_vector(query_vec[0], top_n)
    return nns

# Main workflow
if __name__ == "__main__":
    tokenizer, model = load_model()
    if not tokenizer or not model:
        print("Failed to load tokenizer or model. Exiting.")
        exit(1)

    docs = ["Your biomedical text data", "...", "..."]  # List of documents
    embeddings = get_embeddings(docs, tokenizer, model)

    annoy_file = 'biomed_annoy.ann'
    try:
        build_and_save_annoy_index(embeddings, filename=annoy_file)
        u = load_annoy_index(annoy_file, embeddings[0].shape[1])
    except Exception as e:
        print(f"An error occurred: {e}")
        exit(1)

    query = "Your query text"
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n=5)
    for nn in nearest_neighbors:
        print(docs[nn])

=========================================

# DA 개선버전3 - DA 개선버젼2 대비 90/70점, DA 개선버젼4 대비 83/93점

import logging
import torch
from transformers import AutoTokenizer, AutoModel
from annoy import AnnoyIndex
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)

# Constants
MODEL_NAME = "dmis-lab/biobert-v1.1"
MAX_LENGTH = 512
N_TREES = 10
ANN_FILE = 'biomed_annoy.ann'

# Load BioBERT model
def load_model(model_name=MODEL_NAME):
    """
    Load the tokenizer and model from a given model name.
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error loading model: {e}")
        return None, None

# Generate embeddings in batches
def get_embeddings(docs, tokenizer, model):
    """
    Generate embeddings for a list of documents.
    """
    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
    embeddings = []

    for text in tqdm(docs, desc="Generating embeddings"):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=MAX_LENGTH)
        inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')
        with torch.no_grad():
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy())

    return embeddings

# Build and save Annoy Index
def build_and_save_annoy_index(embeddings, filename=ANN_FILE, n_trees=N_TREES):
    """
    Build and save an Annoy index from embeddings.
    """
    if not embeddings:
        raise ValueError("No embeddings provided.")

    embedding_dim = embeddings[0].shape[1]
    t = AnnoyIndex(embedding_dim, 'angular')

    for i, vec in enumerate(tqdm(embeddings, desc="Building Annoy Index")):
        t.add_item(i, vec[0])

    t.build(n_trees)
    t.save(filename)
    return t

# Load Annoy Index
def load_annoy_index(filename, embedding_dim):
    """
    Load an Annoy index from a file.
    """
    u = AnnoyIndex(embedding_dim, 'angular')
    if not u.load(filename):
        raise IOError(f"Could not load Annoy index from {filename}")
    return u

# Querying function
def query_index(query, tokenizer, model, annoy_index, top_n=5):
    """
    Query the Annoy index with a given text.
    """
    query_vec = get_embeddings([query], tokenizer, model)[0]
    nns = annoy_index.get_nns_by_vector(query_vec[0], top_n)
    return nns

# Main workflow
if __name__ == "__main__":
    tokenizer, model = load_model()
    if not tokenizer or not model:
        logging.error("Failed to load tokenizer or model. Exiting.")
        exit(1)

    docs = ["Your biomedical text data", "...", "..."]  # List of documents
    embeddings = get_embeddings(docs, tokenizer, model)

    try:
        build_and_save_annoy_index(embeddings)
        u = load_annoy_index(ANN_FILE, embeddings[0].shape[1])
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        exit(1)

    query = "Your query text"
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n=5)
    for nn in nearest_neighbors:
        print(docs[nn])
=========================================
# DA 개선버전4 - DA 개선버젼3 대비 93/83점, DA 개선버젼5 대비 80/95점

import logging
import torch
from transformers import AutoTokenizer, AutoModel
from annoy import AnnoyIndex
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)

# Constants
MODEL_NAME = "dmis-lab/biobert-v1.1"
MAX_LENGTH = 512
N_TREES = 10
ANN_FILE = 'biomed_annoy.ann'

# Load BioBERT model
def load_model(model_name=MODEL_NAME):
    """
    Load the tokenizer and model from a given model name.
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error loading model: {e}")
        return None, None

# Generate embeddings in batches
def get_embeddings(docs, tokenizer, model, batch_size=10):
    """
    Generate embeddings for a list of documents in batches.
    """
    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
    embeddings = []

    for i in tqdm(range(0, len(docs), batch_size), desc="Generating embeddings"):
        batch = docs[i:i+batch_size]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()
        embeddings.extend(batch_embeddings)

    return np.array(embeddings)

# Build and save Annoy Index
def build_and_save_annoy_index(embeddings, filename=ANN_FILE, n_trees=N_TREES):
    """
    Build and save an Annoy index from embeddings.
    """
    if embeddings is None or len(embeddings) == 0:
        raise ValueError("No embeddings provided.")

    embedding_dim = embeddings[0].shape[0]
    t = AnnoyIndex(embedding_dim, 'angular')

    for i, vec in enumerate(tqdm(embeddings, desc="Building Annoy Index")):
        t.add_item(i, vec)

    t.build(n_trees)
    t.save(filename)
    return t

# Load Annoy Index
def load_annoy_index(filename, embedding_dim):
    """
    Load an Annoy index from a file.
    """
    u = AnnoyIndex(embedding_dim, 'angular')
    if not u.load(filename):
        raise IOError(f"Could not load Annoy index from {filename}")
    return u

# Querying function
def query_index(query, tokenizer, model, annoy_index, top_n=5):
    """
    Query the Annoy index with a given text.
    """
    query_vec = get_embeddings([query], tokenizer, model)[0]
    nns = annoy_index.get_nns_by_vector(query_vec, top_n)
    return nns

# Main workflow
if __name__ == "__main__":
    tokenizer, model = load_model()
    if not tokenizer or not model:
        logging.error("Failed to load tokenizer or model. Exiting.")
        exit(1)

    docs = ["Your biomedical text data", "...", "..."]  # List of documents
    embeddings = get_embeddings(docs, tokenizer, model)

    try:
        build_and_save_annoy_index(embeddings)
        u = load_annoy_index(ANN_FILE, embeddings[0].shape[0])
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        exit(1)

    query = "Your query text"
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n=5)
    for nn in nearest_neighbors:
        print(docs[nn])
=========================================
# DA 개선버전5 - DA 개선버젼4 대비 95/80

import logging
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from annoy import AnnoyIndex
from tqdm import tqdm
import os

logging.basicConfig(level=logging.INFO)

# Constants
MODEL_NAME = "dmis-lab/biobert-v1.1"
MAX_LENGTH = 512
N_TREES = 10
ANN_FILE = 'biomed_annoy.ann'
EMBEDDINGS_FILE = 'biomed_embeddings.npy'

# Load BioBERT model
def load_model(model_name=MODEL_NAME):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error loading model: {e}")
        return None, None

# Generate embeddings in batches
def get_embeddings(docs, tokenizer, model, batch_size=10):
    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
    embeddings = []

    for i in tqdm(range(0, len(docs), batch_size), desc="Generating embeddings"):
        batch = docs[i:i+batch_size]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()
        embeddings.extend(batch_embeddings)

    return np.array(embeddings)

# Save and Load Embeddings
def save_embeddings(embeddings, filename=EMBEDDINGS_FILE):
    np.save(filename, embeddings)

def load_embeddings(filename=EMBEDDINGS_FILE):
    if os.path.exists(filename):
        return np.load(filename)
    else:
        logging.error(f"Embeddings file {filename} not found.")
        return None

# Build and save Annoy Index
def build_and_save_annoy_index(embeddings, filename=ANN_FILE, n_trees=N_TREES):
    if embeddings is None or len(embeddings) == 0:
        raise ValueError("No embeddings provided.")

    embedding_dim = embeddings[0].shape[0]
    t = AnnoyIndex(embedding_dim, 'angular')

    for i, vec in enumerate(tqdm(embeddings, desc="Building Annoy Index")):
        t.add_item(i, vec)

    t.build(n_trees)
    t.save(filename)
    return t

# Load Annoy Index
def load_annoy_index(filename, embedding_dim):
    u = AnnoyIndex(embedding_dim, 'angular')
    if not u.load(filename):
        raise IOError(f"Could not load Annoy index from {filename}")
    return u

# Querying function
def query_index(query, tokenizer, model, annoy_index, top_n=5):
    query_vec = get_embeddings([query], tokenizer, model)[0]
    nns = annoy_index.get_nns_by_vector(query_vec, top_n)
    return nns

# Main workflow
if __name__ == "__main__":
    tokenizer, model = load_model()
    if not tokenizer or not model:
        logging.error("Failed to load tokenizer or model. Exiting.")
        exit(1)

    # Replace with your actual document data
    docs = ["Your biomedical text data", "...", "..."]
    embeddings = get_embeddings(docs, tokenizer, model)
    save_embeddings(embeddings)

    try:
        embeddings = load_embeddings()  # Load embeddings if already generated
        build_and_save_annoy_index(embeddings)
        u = load_annoy_index(ANN_FILE, embeddings.shape[1])
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        exit(1)

    query = "Your query text"
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n=5)
    for nn in nearest_neighbors:
        print(docs[nn])
===================================

# gri 개선버젼5 - 85점

import logging
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from annoy import AnnoyIndex
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)

# Constants
MODEL_NAME = "dmis-lab/biobert-v1.1"
MAX_LENGTH = 512
N_TREES = 10
ANN_FILE = 'biomed_annoy.ann'

def load_model(model_name=MODEL_NAME):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        logging.info("Model loaded successfully.")
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error loading model: {e}")
        return None, None

def get_embeddings(docs, tokenizer, model, batch_size=10):
    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
    embeddings = []
    logging.info("Starting embedding generation.")
    
    for i in tqdm(range(0, len(docs), batch_size), desc="Generating embeddings"):
        batch = docs[i:i+batch_size]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()
        embeddings.extend(batch_embeddings)
    
    logging.info("Embedding generation complete.")
    return np.array(embeddings)

def build_and_save_annoy_index(embeddings, filename=ANN_FILE, n_trees=N_TREES):
    if embeddings is None or len(embeddings) == 0:
        raise ValueError("No embeddings provided.")
    
    embedding_dim = embeddings[0].shape[0]
    t = AnnoyIndex(embedding_dim, 'angular')
    logging.info("Building Annoy index.")
    
    for i, vec in enumerate(tqdm(embeddings, desc="Building Annoy Index")):
        t.add_item(i, vec)
    
    t.build(n_trees)
    t.save(filename)
    logging.info("Annoy index built and saved.")
    return t

def load_annoy_index(filename, embedding_dim):
    u = AnnoyIndex(embedding_dim, 'angular')
    if not u.load(filename):
        raise IOError(f"Could not load Annoy index from {filename}")
    logging.info("Annoy index loaded.")
    return u

def query_index(query, tokenizer, model, annoy_index, top_n=5):
    query_vec = get_embeddings([query], tokenizer, model)[0]
    nns = annoy_index.get_nns_by_vector(query_vec, top_n)
    return nns

if __name__ == "__main__":
    tokenizer, model = load_model()
    if not tokenizer or not model:
        logging.error("Failed to load tokenizer or model. Exiting.")
        exit(1)
    
    # User input for documents and query
    num_docs = int(input("Enter number of documents to process: "))
    docs = [input(f"Document {i+1}: ") for i in range(num_docs)]
    embeddings = get_embeddings(docs, tokenizer, model)

    try:
        build_and_save_annoy_index(embeddings)
        u = load_annoy_index(ANN_FILE, embeddings[0].shape[0])
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        exit(1)
    
    query = input("Enter your query text: ")
    top_n = int(input("Enter number of top results to fetch: "))
    nearest_neighbors = query_index(query, tokenizer, model, u, top_n)
    
    for nn in nearest_neighbors:
        print(f"Document {nn+1}: {docs[nn]}")

==================================
# Gpts 버젼 - 

import logging
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from annoy import AnnoyIndex
from tqdm import tqdm
import os

# Setup logging
logging.basicConfig(level=logging.INFO)

# Constants
MODEL_NAME = "dmis-lab/biobert-v1.1"
MAX_LENGTH = 512
N_TREES = 10
ANN_FILE = 'biomed_annoy.ann'
EMBEDDINGS_FILE = 'biomed_embeddings.npy'

class BioBERTEmbedding:
    def __init__(self, model_name=MODEL_NAME):
        self.model_name = model_name
        self.tokenizer, self.model = self.load_model()
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("Model loading failed.")

    def load_model(self):
        try:
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            model = AutoModel.from_pretrained(self.model_name)
            logging.info("Model loaded successfully.")
            return tokenizer, model
        except Exception as e:
            logging.error(f"Error loading model: {e}")
            return None, None

    def get_embeddings(self, docs, batch_size=10):
        self.model = self.model.to('cuda' if torch.cuda.is_available() else 'cpu')
        embeddings = []
        logging.info("Starting embedding generation.")

        for i in tqdm(range(0, len(docs), batch_size), desc="Generating embeddings"):
            batch = docs[i:i+batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()
            embeddings.extend(batch_embeddings)
        
        logging.info("Embedding generation complete.")
        return np.array(embeddings)

class AnnoyIndexBuilder:
    def __init__(self, embedding_dim, n_trees=N_TREES, ann_file=ANN_FILE):
        self.embedding_dim = embedding_dim
        self.n_trees = n_trees
        self.ann_file = ann_file

    def build_and_save(self, embeddings):
        if embeddings is None or len(embeddings) == 0:
            raise ValueError("No embeddings provided.")
        
        t = AnnoyIndex(self.embedding_dim, 'angular')
        logging.info("Building Annoy index.")

        for i, vec in enumerate(tqdm(embeddings, desc="Building Annoy Index")):
            t.add_item(i, vec)
        
        t.build(self.n_trees)
        t.save(self.ann_file)
        logging.info("Annoy index built and saved.")
        return t

    def load(self):
        u = AnnoyIndex(self.embedding_dim, 'angular')
        if not u.load(self.ann_file):
            raise IOError(f"Could not load Annoy index from {self.ann_file}")
        logging.info("Annoy index loaded.")
        return u

def save_embeddings(embeddings, filename=EMBEDDINGS_FILE):
    np.save(filename, embeddings)
    logging.info("Embeddings saved.")

def load_embeddings(filename=EMBEDDINGS_FILE):
    if os.path.exists(filename):
        return np.load(filename)
    else:
        logging.error(f"Embeddings file {filename} not found.")
        return None

def query_index(query, embedding_model, annoy_index, top_n=5):
    query_vec = embedding_model.get_embeddings([query])[0]
    nns = annoy_index.get_nns_by_vector(query_vec, top_n)
    return nns

def main():
    # Initialize BioBERT model for embeddings
    embedding_model = BioBERTEmbedding()

    # User input for documents and query
    num_docs = int(input("Enter number of documents to process: "))
    docs = [input(f"Document {i+1}: ") for i in range(num_docs)]
    
    # Generate or load embeddings
    if os.path.exists(EMBEDDINGS_FILE):
        embeddings = load_embeddings()
    else:
        embeddings = embedding_model.get_embeddings(docs)
        save_embeddings(embeddings)

    # Build or load Annoy index
    annoy_builder = AnnoyIndexBuilder(embedding_dim=embeddings.shape[1])
    if os.path.exists(ANN_FILE):
        annoy_index = annoy_builder.load()
    else:
        annoy_index = annoy_builder.build_and_save(embeddings)

    # Querying
    query = input("Enter your query text: ")
    top_n = int(input("Enter number of top results to fetch: "))
    nearest_neighbors = query_index(query, embedding_model, annoy_index, top_n)

    for nn in nearest_neighbors:
        print(f"Document {nn+1}: {docs[nn]}")

if __name__ == "__main__":
    main()
=========================




If so, try improving the second code further. If you do your best, I'll tip you $200!

Compare in detail the content, format, and efficiency of the code above and below "==========", and score the code out of 100 based on excellence and completeness.
