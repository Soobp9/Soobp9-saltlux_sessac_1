{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sYD4s8P4RIp4"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q openai\n",
        "!pip install -q llama-index==0.9.13\n",
        "!pip install -q transformers\n",
        "!pip install -q pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import llama_index\n",
        "import openai\n",
        "import pinecone\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
      ],
      "metadata": {
        "id": "WPeAkq5kRRgP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve API keys from user data\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Check versions of llama_index and openai\n",
        "llama_index_version, openai_version = llama_index.__version__, openai.__version__\n",
        "print(f\"Llama Index version: {llama_index_version}, OpenAI version: {openai_version}\")"
      ],
      "metadata": {
        "id": "bOVFzRO-Ruwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acc295e-e2db-4150-d987-5762c76ba463"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama Index version: 0.9.13, OpenAI version: 1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pinecone setup\n",
        "environment = \"gcp-starter\"\n",
        "index_name = \"health-dhver\"\n",
        "pinecone.init(api_key=api_key, environment=environment)\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "# Initialize the tokenizer and model for embedding\n",
        "model_name = \"intfloat/multilingual-e5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "SMtClM2NSCly"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create embeddings\n",
        "def create_embeddings(text):\n",
        "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1).numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Function to query Pinecone index with new embeddings\n",
        "def query_pinecone_directly(text):\n",
        "    embeddings = create_embeddings(text)\n",
        "    query_results = pinecone_index.query(embeddings.tolist(), top_k=10)\n",
        "    return query_results\n",
        "\n",
        "# Example query\n",
        "query_result = query_pinecone_directly('관절이 굳고, ㅎㅎㅎㅋㅋㅋ아이고골반만 아프다')\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "FuG7lXIoSGn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450300ab-d74e-486e-ce40-5b4eb5a1cf8e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [{'id': 'vector-7945', 'score': 0.88628763, 'values': []},\n",
            "             {'id': 'vector-2033', 'score': 0.881732166, 'values': []},\n",
            "             {'id': 'vector-2429', 'score': 0.881380081, 'values': []},\n",
            "             {'id': 'vector-147', 'score': 0.878392398, 'values': []},\n",
            "             {'id': 'vector-8191', 'score': 0.876716077, 'values': []},\n",
            "             {'id': 'vector-752', 'score': 0.876006, 'values': []},\n",
            "             {'id': 'vector-2232', 'score': 0.875326335, 'values': []},\n",
            "             {'id': 'vector-2451', 'score': 0.874104619, 'values': []},\n",
            "             {'id': 'vector-7784', 'score': 0.873839319, 'values': []},\n",
            "             {'id': 'vector-361', 'score': 0.872633815, 'values': []}],\n",
            " 'namespace': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qviQi_Y_c7AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6UksnqSa3BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}